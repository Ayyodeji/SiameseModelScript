# -*- coding: utf-8 -*-
"""SiameseWorkFinalWorkflowZX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qpjtJiIDzMh71mqUgVM-eDastz1GcXHn
"""

import os
import cv2
import time
import random
import pandas as pd
import math
import numpy as np
from matplotlib import pyplot
import tensorflow as tf
import scipy.stats
from keras.applications.inception_v3 import preprocess_input
from keras import backend, layers, metrics
from keras.optimizers import Adam
from keras.applications import Xception, ResNet50, DenseNet121, InceptionV3, DenseNet201
from keras.models import Model, Sequential

from sklearn.metrics.pairwise import cosine_similarity
from numpy.linalg import norm
import seaborn as sns
import matplotlib.pyplot as plt
tf.__version__, np.__version__
import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""##Function"""

def get_encoder(input_shape):
    """ Returns the image encoding model """

    # RESNET50
    pretrained_model = ResNet50(
        input_shape=input_shape,
        weights='imagenet',
        include_top=False,
        pooling='avg',
    )
    for i in range(len(pretrained_model.layers)-27):
        pretrained_model.layers[i].trainable = False

    encode_model = Sequential([
        pretrained_model,
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(256, activation="relu"),
        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))
    ], name="Encode_Model")
    return encode_model
    
class DistanceLayer(layers.Layer):
    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, anchor, positive, negative):
        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)
        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)
        return (ap_distance, an_distance)
    

def get_siamese_network(input_shape = (128, 128, 3)):
    encoder = get_encoder(input_shape)
    
    # Input Layers for the images
    anchor_input   = layers.Input(input_shape, name="Anchor_Input")
    positive_input = layers.Input(input_shape, name="Positive_Input")
    negative_input = layers.Input(input_shape, name="Negative_Input")
    
    ## Generate the encodings (feature vectors) for the images
    encoded_a = encoder(anchor_input)
    encoded_p = encoder(positive_input)
    encoded_n = encoder(negative_input)
    
    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²
    distances = DistanceLayer()(
        encoder(anchor_input),
        encoder(positive_input),
        encoder(negative_input)
    )
    
    # Creating the Model
    siamese_network = Model(
        inputs  = [anchor_input, positive_input, negative_input],
        outputs = distances,
        name = "Siamese_Network"
    )
    return siamese_network

siamese_network = get_siamese_network()
class SiameseModel(Model):
    # Builds a Siamese model based on a base-model
    def __init__(self, siamese_network, margin=1.0):
        super(SiameseModel, self).__init__()
        
        self.margin = margin
        self.siamese_network = siamese_network
        self.loss_tracker = metrics.Mean(name="loss")

    def call(self, inputs):
        return self.siamese_network(inputs)

    def train_step(self, data):
        # GradientTape get the gradients when we compute loss, and uses them to update the weights
        with tf.GradientTape() as tape:
            loss = self._compute_loss(data)
            
        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)
        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))
        
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

    def test_step(self, data):
        loss = self._compute_loss(data)
        
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

    def _compute_loss(self, data):
        # Get the two distances from the network, then compute the triplet loss
        ap_distance, an_distance = self.siamese_network(data)
        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)
        return loss

    @property
    def metrics(self):
        # We need to list our metrics so the reset_states() can be called automatically.
        return [self.loss_tracker]
siamese_model = SiameseModel(siamese_network)

optimizer = Adam(learning_rate=1e-3, epsilon=1e-01)
siamese_model.compile(optimizer=optimizer)
siamese_model.load_weights("/content/drive/My Drive/siamese_model_final")

def extract_encoder(model):
    encoder = get_encoder((128, 128, 3))
    i=0
    for e_layer in model.layers[0].layers[3].layers:
        layer_weight = e_layer.get_weights()
        encoder.layers[i].set_weights(layer_weight)
        i+=1
    return encoder

encoder = extract_encoder(siamese_model)
encoder.save_weights("encoder")

def predict_similarity(image1, image2, threshold=0.8):
    img1 = []
    img2 = []
    image_1 = cv2.imread(image1)
    image_2 = cv2.imread(image2)

    image_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2RGB)
    image_1 = cv2.resize(image_1, (128,128))
    
    image_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2RGB)
    image_2 = cv2.resize(image_2, (128,128))

    img1.append(image_1)
    img2.append(image_2)
    img1 = preprocess_input(np.array(img1))
    img2 = preprocess_input(np.array(img2))

    tensor1 = encoder.predict(img1)
    tensor2 = encoder.predict(img2)

    correlation, _ = scipy.stats.spearmanr(tensor1, tensor2)
    distance = 1 - correlation

    threshold = 0.5  # Set your desired threshold here
    prediction = np.where(distance <= threshold, 0, 1)
    distance *= 100
    return str(str(distance[0]) + '%')
def bulktest(dirlist):
  arrBT = []
  arrBTN = []
  for cls in os.listdir(dirlist):
    for i in os.listdir(dirlist):
      if cls!=i:
        
        arrBTN.append(str(i).split(".")[0] + '   vs   ' + str(cls).split(".")[0]) 
        arrBT.append(predict_similarity( os.path.join(dirlist,i), os.path.join(dirlist,cls) ))

  df = pd.DataFrame({'Comparison': arrBTN, 'Result': arrBT})
  df = df.drop_duplicates(subset=['Result'])
  df = df.sort_values('Comparison')
  return df

def plottest(dirlist):
  for i in os.listdir(dirlist):
    for cls in os.listdir(dirlist):
      if i!=cls:
        pyplot.figure(figsize=(10,3))
        pyplot.subplot(1, 2, 1)
        filename = os.path.join(dirlist,i)
        image = cv2.imread(filename)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (128,128))
        
        pyplot.imshow(image)
        pyplot.title(str(i).split(".")[0])
        sns.set_style('white')
        pyplot.text(330.5, 150.5, "Siimilarity :  " + str((predict_similarity( os.path.join(dirlist,i), os.path.join(dirlist,cls) ))))

        pyplot.subplot(1, 2, 2)
        filename = os.path.join(dirlist,cls)
        image = cv2.imread(filename)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (128,128))
        pyplot.imshow(image)
        pyplot.title(str(cls).split(".")[0])
        pyplot.show()

dir = '/content/drive/My Drive/LabeledData/'

def rmvDS_Store(dir):
  if os.path.exists(os.path.join(dir + '.DS_Store')):
    os.remove(os.path.join(dir + '.DS_Store'))
    print('DS Store removed')
  else:
    print('DS Store does not exist')
rmvDS_Store(dir)

"""### CREATE LABEL FRAME"""

# Commented out IPython magic to ensure Python compatibility.
# label_dictionary = []
# label_encoding = []
# 
# 
# for i in os.listdir(dir):
#     img = os.path.join(dir,(str(i) + '/0.jpg'))
#     try:
#         img = cv2.imread(img)
#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#     except cv2.error as e:
#         print(f"OpenCV error: {e}")
#         continue  # skip this image and move on to the next one
# 
#     img = cv2.resize(img, (128,128))
#     img = preprocess_input(np.array([img]))
#     imgEncoding = encoder.predict(img)
#     label_encoding.append(imgEncoding)
#     label_dictionary.append(i)
# 
# 
# LabelDataframe = pd.DataFrame({'Label': label_dictionary, 'encoding': label_encoding})
# LabelDataframe.to_csv('labelEncoding.csv', index=False)
# 
# # download the CSV file from Colab
# from google.colab import files
# files.download('labelEncoding.csv')

"""### SAVE LABEL FRAME"""

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import auth
# from googleapiclient.discovery import build
# from googleapiclient.errors import HttpError
# import pandas as pd
# import io
# from datetime import datetime
# from googleapiclient.http import MediaIoBaseDownload
# import logging
# logging.getLogger('googleapiclient.discovery').setLevel(logging.ERROR)
# 
# auth.authenticate_user()
# drive_service = build('drive', 'v3')
# 
# filename = '_LabelEncoding.csv'
# LabelDataframe.to_csv(filename, index=False)
# 
# # upload the CSV file to Google Drive
# file_metadata = {'name': filename, 'parents': ['1y6XGoy_G9fgLK62sxoh0LObeOOQE9oMR']}
# media = drive_service.files().create(
#     body=file_metadata,
#     media_body=filename,
#     fields='id'
# ).execute()
# 
# # download the CSV file from Colab to Google Drive
# file_id = media.get('id')
# request = drive_service.files().get_media(fileId=file_id)
# content = io.BytesIO()
# downloader = MediaIoBaseDownload(content, request)
# done = False
# while done is False:
#     status, done = downloader.next_chunk()
#     print(f"Download {int(status.progress() * 100)}.")
# content.seek(0)
# 
# # create a new file in Google Drive with the downloaded content
# file_metadata = {'name': filename, 'parents': ['1y6XGoy_G9fgLK62sxoh0LObeOOQE9oMR']}
# # file = drive_service.files().create(body=file_metadata, media_body=content, fields='id').execute()
#

"""### EXPERIMENT"""

# Commented out IPython magic to ensure Python compatibility.
# label_dictionary = []
# img_dictionary = []
# label_encode = []
# imgarr = []
# for i in os.listdir(dir):
# 
#   for j in os.listdir(os.path.join(dir,(str(i)))):
#     img = os.path.join(dir,(str(i) + "/" + str(j)))
#     try:
#         img = cv2.imread(img)
#         img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#     except cv2.error as e:
#         print(f"OpenCV error: {e}")
#         continue  # skip this image and move on to the next one
# 
#     img = cv2.resize(img, (128,128))
#     img = np.stack((img,)*3, axis=-1)
#     img = preprocess_input(np.array([img]))
#     imgEncoding = encoder.predict(img)
#     img_dictionary.append(imgEncoding)
# 
#     arr_sum = np.sum(img_dictionary, axis=0)
#     
#     # Divide the sum by the number of arrays
#     arr_avg = arr_sum / len(img_dictionary)
#     print(arr_avg.shape)
#     label_encode.append(arr_avg[0])
#     label_dictionary.append(i)
# 
# 
# LabelDataframe = pd.DataFrame({'Label': label_dictionary, 'encoding': label_encoding})
# LabelDataframe.to_csv('labelEncoding.csv', index=False)
# 
# # download the CSV file from Colab
# from google.colab import files
# files.download('labelEncoding.csv')

"""### STOP"""

BackUpDF = LabelDataframe

LabelDataframe.info()

LabelDataframe = pd.read_csv('/content/drive/My Drive/LabelEncoding CSV/_LabelEncoding.csv')
LabelDataframe.info()

"""## LABELLING"""

def predict_label(image, threshold=0.98):
  img = image
  img = cv2.imread(img)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  img = cv2.resize(img, (128,128))
  img = preprocess_input(np.array([img]))
  imgEncoding = encoder.predict(img)

  _arr = []
  arr = []
  for i in LabelDataframe.index:
    _arr.append(LabelDataframe.Label[i])
    tensor1 = LabelDataframe.encoding[i].astype(float)
    tensor2 = imgEncoding
    distance = np.sum(tensor1*tensor2, axis=1)/(norm(tensor1, axis=1)*norm(tensor2, axis=1))
    arr.append(distance)
  tempdf = pd.DataFrame({'Label': _arr, 'Result': arr})
  tempdf = tempdf.sort_values('Result', ascending=False)
  tempdf = tempdf.reset_index(drop=True)
  label = tempdf.Label[0]
  print(tempdf.head(40))
  print(tempdf.Result[0])
  if tempdf.Result[0] >= threshold:
    print(label)
  else:
    print('Not Found')
#
#predict_label('/content/drive/My Drive/photo.jpg')


"""### ADD NEW GENE"""

def addGene(name, newname, label, encimg ):
  tempdf = os.path.join('/content/drive/My Drive/LabelEncoding CSV' + name)
  tempimg = os.path.join('/content/drive/My Drive/LabelEncoding CSV',str(encimg))

  try:
      img = cv2.imread(tempimg)
      img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  except cv2.error as e:
      print(f"OpenCV error: {e}")
      print('Try Again')

  img = cv2.resize(img, (128,128))
  img = np.stack((img,)*3, axis=-1)
  img = preprocess_input(np.array([img]))
  imgEncoding = encoder.predict(img)
  new_rows = [{'Label': str(label),'Encoding': imgEncoding }]
  new_df = pd.DataFrame(new_rows)
  tempdf = tempdf.append(new_df, ignore_index=True)
  filename = str(newname)
  tempdf.to_csv(os.path.join(str(newname),'.csv'), index=False)

  # upload the CSV file to Google Drive
  file_metadata = {'name': filename, 'parents': ['1y6XGoy_G9fgLK62sxoh0LObeOOQE9oMR']}
  media = drive_service.files().create(
      body=file_metadata,
      media_body=filename,
      fields='id'
  ).execute()

  # download the CSV file from Colab to Google Drive
  file_id = media.get('id')
  request = drive_service.files().get_media(fileId=file_id)
  content = io.BytesIO()
  downloader = MediaIoBaseDownload(content, request)
  done = False
  while done is False:
      status, done = downloader.next_chunk()
      print(f"Download {int(status.progress() * 100)}.")
  content.seek(0)

  # create a new file in Google Drive with the downloaded content
  file_metadata = {'name': filename, 'parents': ['1y6XGoy_G9fgLK62sxoh0LObeOOQE9oMR']}



#########Singletest takes two files#############
#########Bulk Test Takes a directory############
#########Predict label takes a file#############




"""#SINGLE TEST FUNCTION"""

#geneImage1 = input("Enter Image 1 Path: ")  #
#geneImage2 = input("Enter Image 2 Path: ")  #
#predict_similarity(geneImage1,geneImage2)

"""#BULK TEST FUNCTION"""
# genImageDir = input("Enter Image 1 Path: ")
# bulktest(genImageDir)



"""#LABEL TEST FUNCTION"""
#geneImage1 = input("Enter Image 1 Path: ")  #
#predict_label(geneImage1)
